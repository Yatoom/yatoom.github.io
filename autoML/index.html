<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					## OpenML Robot Assistant for algorithm selection and hyperparameter tuning
					Jeroen van Hoof (0778486),
					**supervisor**: Dr. ir. J. Vanschoren
				</section>
				<section data-markdown>
					## The problem
					- Machine learning relies on choosing the right algorithm, associated parameters and deciding on features
					- We don't know which algorithms and parameters work well
					- Requires a ton of manual programming and brute forcing
					- Combined Algorithm Selection and Hyperparameter (CASH)
				</section>
				<section>
					<section data-markdown>
						## Hyperparameter tuning
					</section>
					<section data-markdown>
						### Grid search and Randomized search
						![](img/searches.jpeg)
						</section>
						<section data-markdown>
							### Grid search and Randomized search
							- Grid search
								- Pre-specified parameters
								- Brute force
								- Not possible to make tradeoff between computation time and accuracy
								- Does not work with continuous parameters
							- Randomized search
								- Works with continous parameters
								- Can be terminated at any point in time
						</section>
						<section data-markdown>
							### Bayesian optimization
							- Based on Bayes' Theorem
							- Treat outcome of configurations as random function
								- Normal distribution &rarr; Gaussian processes
							- Draw a line with prior belief of the objective function
							- After gathering evaluation outcome, prior is updated
						</section>
						<section>
							<img src="img/bayesian.png" height="600px">
						</section>
						<section data-markdown>
							![](img/random_vs_bayesian.png)
							- Random search: parallizable, efficient enough for most tasks, but unreliable for training DBNs
							- Bayesian search: improved accuracy
						</section>
						<section data-markdown>
							### Hyperbandit
							- Bayesian optimization: adaptively choose new configurations
							- Hyperbandit: adaptively allocate resources across the selected configurations
							- Split hyper-parameter space into many parts, do evaluations, throw out worst half, reallocate resources to remaining half
							- Principle: if a hyperparameter configuration is destined to be the best after a large number of iterations, it is more likely than not to perform in the top half of configurations after a small number of iterations
						</section>
						<section data-markdown>
							![](img/benchmark.png)
							Hyperband seems to work well with less resources
						</section>
				</section>
				<section>
					<section data-markdown>
						## Algorithm selection
					</section>
					<section data-markdown>
						### Methods
						- SMBO (Sequential Model Based Optimization)
							- Try out all algorithms
							- Use any state-of-the-art hyperparameter optimization method to find best algorithm
						- Genetic programming
							- Evolve the sequence of pipeline operators as well as each operatorâ€™s parameters
						- Meta-learning
							- Learn from past experiences about which algorithms and hyper-parameter spaces are likely to work well
							- Can quickly suggest some instantiations, but unable to provide fine-grained information on performance
							- Warmstart the Bayesian optimizer
					</section>
					<section data-markdown>
						### A few implementations
						- Auto-Weka
							- Based on SMBO (uses SMAC implementation with random forests)
							- Uses Bayesian optimization on WEKA (a collection of machine learning algorithms for data mining tasks)
						- Auto-Sklearn
							- Similar to Auto-Weka, but for scikit-learn
							- Uses Meta-learning for warmstarting
							- Automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator
						- TPOT
							- Based on Genetic Programming
							- Python tool that automatically creates and optimizes machine learning pipelines using genetic programming
					</section>
					<section data-markdown>
						### TPOT (mini demo)
						![](img/tpot-demo.gif)
					</section>
				</section>
				<section>
					<section data-markdown>
						## Focus
					</section>
					<section data-markdown>
						![](img/slide-1.png)
					</section>
					<section data-markdown>
						![](img/slide-2.png)
					</section>
					<section>
						<blockquote>"A robot that automatically executes scikit-learn benchmarks for new datasets"</blockquote>
						<img src="img/robots.png" style="border: none; background: transparent">
					</section>
					<section>
						<h3>OpenML</h3>
						<iframe data-src="http://www.openml.org/d/554" width="100%" height="600px">OpenML</iframe>
					</section>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
